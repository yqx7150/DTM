2025-04-11 12:10:47,673 INFO: 
     ______                   __   __                 __      __
    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /
   / / __ / __ \ / __ \ / __  /  / /   / / / // ___// //_/  / /
  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/
  \____/ \____/ \____/ \____/  /_____/\____/ \___//_/|_|  (_)
    
Version Information: 
	BasicSR: 1.4.2
	PyTorch: 2.2.1
	TorchVision: 0.17.1
2025-04-11 12:10:47,673 INFO: 
  name: train_DTMS1
  model_type: DTMS1Model
  scale: 1
  num_gpu: 1
  manual_seed: 0
  datasets:[
    train:[
      name: TrainSet
      type: DeblurPairedDataset
      dataroot_gt: /home/b109/lxb/DiffIR-master/DiffIR-demotionblur/dataset/train/full
      dataroot_lq: /home/b109/lxb/DiffIR-master/DiffIR-demotionblur/dataset/train/low
      geometric_augs: True
      filename_tmpl: {}
      io_backend:[
        type: disk
      ]
      num_worker_per_gpu: 1
      batch_size_per_gpu: 2
      mini_batch_sizes: [4]
      iters: [300000]
      gt_size: 256
      gt_sizes: [256]
      use_shuffle: True
      dataset_enlarge_ratio: 1
      prefetch_mode: None
      phase: train
      scale: 1
    ]
    val:[
      name: ValSet
      type: DeblurPairedDataset
      dataroot_gt: /home/b109/lxb/DiffIR-master/DiffIR-demotionblur/dataset/val/full
      dataroot_lq: /home/b109/lxb/DiffIR-master/DiffIR-demotionblur/dataset/val/low
      io_backend:[
        type: disk
      ]
      phase: val
      scale: 1
    ]
  ]
  network_g:[
    type: DTMS1
    n_encoder_res: 5
    inp_channels: 3
    out_channels: 3
    dim: 48
    num_blocks: [3, 5, 6, 6]
    num_refinement_blocks: 4
    heads: [1, 2, 4, 8]
    ffn_expansion_factor: 2
    bias: False
    LayerNorm_type: WithBias
  ]
  path:[
    pretrain_network_g: None
    param_key_g: params_ema
    strict_load_g: True
    resume_state: None
    experiments_root: /home/b109/lxb/DiffIR-master/DiffIR-demotionblur/DTM-master/experiments/train_DTMS1
    models: /home/b109/lxb/DiffIR-master/DiffIR-demotionblur/DTM-master/experiments/train_DTMS1/models
    training_states: /home/b109/lxb/DiffIR-master/DiffIR-demotionblur/DTM-master/experiments/train_DTMS1/training_states
    log: /home/b109/lxb/DiffIR-master/DiffIR-demotionblur/DTM-master/experiments/train_DTMS1
    visualization: /home/b109/lxb/DiffIR-master/DiffIR-demotionblur/DTM-master/experiments/train_DTMS1/visualization
  ]
  train:[
    ema_decay: 0.999
    optim_g:[
      type: Adam
      lr: 0.0002
      weight_decay: 0
      betas: [0.9, 0.999]
    ]
    scheduler:[
      type: CosineAnnealingRestartCyclicLR
      periods: [92000, 208000]
      restart_weights: [1, 1]
      eta_mins: [0.000285, 1e-06]
    ]
    total_iter: 300000
    warmup_iter: -1
    mixing_augs:[
      mixup: False
      mixup_beta: 1.2
      use_identity: True
    ]
    pixel_opt:[
      type: L1Loss
      loss_weight: 1.0
      reduction: mean
    ]
  ]
  val:[
    val_freq: 5000.0
    save_img: False
    metrics:[
      psnr:[
        type: calculate_psnr
        crop_border: 0
        test_y_channel: False
      ]
    ]
  ]
  logger:[
    print_freq: 1000
    save_checkpoint_freq: 10000.0
    use_tb_logger: True
    wandb:[
      project: None
      resume_id: None
    ]
  ]
  dist_params:[
    backend: nccl
    port: 29500
  ]
  dist: True
  rank: 0
  world_size: 1
  auto_resume: False
  is_train: True
  root_path: /home/b109/lxb/DiffIR-master/DiffIR-demotionblur/DTM-master

2025-04-11 12:10:51,986 INFO: Dataset [DeblurPairedDataset] - TrainSet is built.
2025-04-11 12:10:51,986 INFO: Training statistics:
	Number of train images: 25986
	Dataset enlarge ratio: 1
	Batch size per gpu: 2
	World size (gpu number): 1
	Require iter number per epoch: 12993
	Total epochs: 24; iters: 300000.
2025-04-11 12:10:51,986 INFO: Dataset [DeblurPairedDataset] - ValSet is built.
2025-04-11 12:10:51,986 INFO: Number of val images/folders in ValSet: 30
2025-04-11 12:10:52,100 INFO: Network [DTMS1] is created.
2025-04-11 12:10:52,346 INFO: Network: DistributedDataParallel - DTMS1, with parameters: 26,742,206
2025-04-11 12:10:52,346 INFO: DTMS1(
  (G): DIRformer(
    (patch_embed): OverlapPatchEmbed(
      (proj): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (encoder_level1): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=96, bias=False)
          )
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=96, bias=False)
          )
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=96, bias=False)
          )
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=96, bias=False)
          )
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=96, bias=False)
          )
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=96, bias=False)
          )
        )
      )
    )
    (down1_2): Downsample(
      (body): Sequential(
        (0): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): PixelUnshuffle(downscale_factor=2)
      )
    )
    (encoder_level2): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
    )
    (down2_3): Downsample(
      (body): Sequential(
        (0): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): PixelUnshuffle(downscale_factor=2)
      )
    )
    (encoder_level3): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
    )
    (down3_4): Downsample(
      (body): Sequential(
        (0): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): PixelUnshuffle(downscale_factor=2)
      )
    )
    (latent): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
          (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
          (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
          (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
          (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
          (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
          (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=768, bias=False)
          )
        )
      )
    )
    (up4_3): Upsample(
      (body): Sequential(
        (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): PixelShuffle(upscale_factor=2)
      )
    )
    (reduce_chan_level3): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (decoder_level3): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
          (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=384, bias=False)
          )
        )
      )
    )
    (up3_2): Upsample(
      (body): Sequential(
        (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): PixelShuffle(upscale_factor=2)
      )
    )
    (reduce_chan_level2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (decoder_level2): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
    )
    (up2_1): Upsample(
      (body): Sequential(
        (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): PixelShuffle(upscale_factor=2)
      )
    )
    (decoder_level1): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
    )
    (refinement): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (kernel): Sequential(
            (0): Linear(in_features=256, out_features=192, bias=False)
          )
        )
      )
    )
    (output): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (E): CPEN(
    (E): Sequential(
      (0): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.1, inplace=True)
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): LeakyReLU(negative_slope=0.1, inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (3): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): LeakyReLU(negative_slope=0.1, inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (4): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): LeakyReLU(negative_slope=0.1, inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (5): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): LeakyReLU(negative_slope=0.1, inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (6): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): LeakyReLU(negative_slope=0.1, inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): LeakyReLU(negative_slope=0.1, inplace=True)
      (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (10): LeakyReLU(negative_slope=0.1, inplace=True)
      (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (12): LeakyReLU(negative_slope=0.1, inplace=True)
      (13): AdaptiveAvgPool2d(output_size=1)
    )
    (mlp): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LeakyReLU(negative_slope=0.1, inplace=True)
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (pixel_unshuffle): PixelUnshuffle(downscale_factor=4)
  )
  (pixel_unshuffle): PixelUnshuffle(downscale_factor=4)
)
2025-04-11 12:10:52,347 INFO: Use Exponential Moving Average with decay: 0.999
2025-04-11 12:10:52,441 INFO: Network [DTMS1] is created.
2025-04-11 12:10:52,506 INFO: Loss [L1Loss] is created.
2025-04-11 12:10:52,508 INFO: Model [DTMS1Model] is created.
2025-04-11 12:10:53,859 INFO: Start training from epoch: 0, iter: 0
2025-04-11 12:10:55,691 INFO: 
 Updating Patch_Size to 256 and Batch_Size to 4 

